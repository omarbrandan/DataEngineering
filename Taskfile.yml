version: '3'

env:
  ROOT_PROJECT: ./workspaces/DataEngineering
  DOCKER: ../../docker-compose.yml

tasks:
  pre_project:
    cmds:
      - echo "CREATE THE AIRLFOW AND THE STORAGE DIRS"
      - mkdir -p $ROOT_PROJECT
      - |
        if [ ! -f $ROOT_PROJECT/.env ]; then
          echo "Creating .env file..."
          echo -e "AIRFLOW_UID=$(id -u)" > $ROOT_PROJECT/.env
          cat <<EOF >> $ROOT_PROJECT/.env
        AIRFLOW_GID=0
        REDSHIFT_URL=redshift+psycopg2://omaragustinbrandan_coderhouse:5A6MEpt8Eg@data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com:5439/data-engineer-database
        REDSHIFT_USER=omaragustinbrandan_coderhouse
        REDSHIFT_PASSWORD=5A6MEpt8Eg
        REDSHIFT_PORT=5439
        REDSHIFT_DBNAME=data-engineer-database
        REDSHIFT_SCHEMA=omaragustinbrandan_coderhouse
        REDSHIFT_HOST=data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com
        EMAIL=omaragustinbrandan@gmail.com
        EMAIL_PASSWORD=jtkwuaaxlqfwkidy
        SMTP_HOST=smtp.gmail.com
        SMTP_STARTTLS=True
        SMTP_SSL=False
        SMTP_USER=omaragustinbrandan@gmail.com
        SMTP_PASSWORD=jtkwuaaxlqfwkidy
        SMTP_PORT=587
        SMTP_MAIL_FROM=omaragustinbrandan@gmail.com
        EOF
        else
          echo ".env file already exists"
        fi
      - source $ROOT_PROJECT/.env
      - mkdir -p $ROOT_PROJECT/{raw_data,processed_data,dags,logs,plugins,config}
      - tree -L 2 $ROOT_PROJECT

  start_project:
      - source $ROOT_PROJECT/.env
      - docker compose -f $ROOT_PROJECT/$DOCKER up airflow-init --build
      - docker compose -f $ROOT_PROJECT/$DOCKER up -d --build
      - docker container ls -a

  down_project:
      - docker compose -f $ROOT_PROJECT/$DOCKER down

  cleanup: 
    cmds:
      - rm -rf  $ROOT_PROJECT/{raw_data,processed_data,logs,plugins,config}
      - rm -f   $ROOT_PROJECT/.env
      - rm -rf  $ROOT_PROJECT/dags/*pycache*
      - rm -rf  $ROOT_PROJECT/dags/*/*pycache*